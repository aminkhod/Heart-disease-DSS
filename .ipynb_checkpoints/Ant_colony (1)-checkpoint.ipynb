{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qfzl0het9og7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "amOHtX8qRc4S"
   },
   "outputs": [],
   "source": [
    "dfSM=pd.read_excel('Actual Data_BalanceData_SMOTE.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VhG4bC7ORdCO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn import svm\n",
    "from numba import njit\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hJhzk7iZRdFN"
   },
   "outputs": [],
   "source": [
    "X=dfSM.drop('Target', axis=1)\n",
    "y=dfSM['Target']\n",
    "from sklearn import preprocessing\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25,\n",
    "                                                    random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bIy1x8SiRdIR",
    "outputId": "5bd10839-afe2-4328-9669-224482c0017c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7241379310344828"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(Xtrain, ytrain)\n",
    "clf.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bKqcQYSGWhJ7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "def update_eta(Xtrain, Xtest, ytrain, ytest, tabu):\n",
    "    \"\"\"更新启发式信息, eta = TPR / d\n",
    "    Returns:\n",
    "        eta: 从tabu最后一个节点出发到其余节点的启发式信息\n",
    "    \"\"\"\n",
    "    n_dims = Xtrain.shape[1]\n",
    "    eta = np.zeros(n_dims)\n",
    "    flist = list(set(range(n_dims)) - set(tabu)) \n",
    "\n",
    "    for i in flist:\n",
    "        clf = GradientBoostingClassifier()\n",
    "        clf.fit(Xtrain.iloc[:, tabu+[i]], ytrain)\n",
    "        pred = clf.predict(Xtest.iloc[:, tabu+[i]])\n",
    "        cf_matrix = confusion_matrix(ytest, pred)\n",
    "        FN, TP = cf_matrix[1][0], cf_matrix[1][1]\n",
    "        eta[i] = TP / (TP + FN) / (len(tabu)+1)\n",
    "\n",
    "    return eta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "T3ayNP3IWjCB"
   },
   "outputs": [],
   "source": [
    "def select_route(prob):\n",
    "    \"\"\"按路径转移概率选择下一个特征\n",
    "    \"\"\"\n",
    "    cs = np.cumsum(prob)\n",
    "    p = np.random.rand()\n",
    "    for i in range(len(cs)):\n",
    "        if cs[i] > p:\n",
    "            break\n",
    "    \n",
    "    return i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zun0I_EuWkwW"
   },
   "outputs": [],
   "source": [
    "def fitness_func(Xtrain, Xtest, ytrain, ytest, selected, omega=0.7):\n",
    "    \"\"\"适应度函数，评估特征子集好坏\n",
    "    Returns:\n",
    "        result: 适应度\n",
    "    \"\"\"\n",
    "    clf = GradientBoostingClassifier()\n",
    "    clf.fit(Xtrain.iloc[:, selected], ytrain)\n",
    "    pred = clf.predict(Xtest.iloc[:, selected])\n",
    "\n",
    "    cf_matrix = confusion_matrix(ytest, pred)\n",
    "    TN, FP = cf_matrix[0][0], cf_matrix[0][1]\n",
    "    FPR = FP / (TN + FP)\n",
    "    f_result = omega*FPR + (1-omega)*(len(selected)/Xtrain.shape[1])\n",
    "    acc = accuracy_score(ytest, pred)\n",
    "    return f_result, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KeUyEezPXKFG"
   },
   "outputs": [],
   "source": [
    "def fitness_func(Xtrain, Xtest, ytrain, ytest, selected, omega=0.8):\n",
    "    \"\"\"适应度函数，评估特征子集好坏\n",
    "    Returns:\n",
    "        result: 适应度\n",
    "    \"\"\"\n",
    "    clf = GradientBoostingClassifier()\n",
    "    clf.fit(Xtrain.iloc[:, selected], ytrain)\n",
    "    pred = clf.predict(Xtest.iloc[:, selected])\n",
    "\n",
    "    cf_matrix = confusion_matrix(ytest, pred)\n",
    "    TN, FP = cf_matrix[0][0], cf_matrix[0][1]\n",
    "    FPR = FP / (TN + FP)\n",
    "\n",
    "    acc = accuracy_score(ytest, pred)\n",
    "    f_result = omega*(1-acc) + (1-omega)*np.exp(len(selected)/Xtrain.shape[1]-2)\n",
    "    # print(FPR, (1-omega)*np.exp(len(selected)/train_data.shape[1]-10), f_result)\n",
    "    return f_result, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qimYASOfXWHo"
   },
   "outputs": [],
   "source": [
    "def init_eta(Xtrain, Xtest, ytrain, ytest):\n",
    "    n_dims = Xtrain.shape[1]\n",
    "    eta = np.zeros([n_dims, n_dims])\n",
    "    for i in tqdm(range(n_dims)):\n",
    "        for j in range(n_dims):\n",
    "            clf = GradientBoostingClassifier()\n",
    "            clf.fit(Xtrain.iloc[:, [i,j]], ytrain)\n",
    "            pred = clf.predict(Xtest.iloc[:, [i,j]])\n",
    "            cf_matrix = confusion_matrix(ytest, pred)\n",
    "            FN, TP = cf_matrix[1][0], cf_matrix[1][1]\n",
    "            eta[i][j] = TP / (TP + FN) /2\n",
    "    \n",
    "    return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MH_RM7-kXhRX",
    "outputId": "612582a7-a601-45b1-aea6-f0e02fd061c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Num of the total features: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/24 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "n_dims = Xtrain.shape[1]\n",
    "n_ants = 5\n",
    "print(\"the Num of the total features:\", n_dims)\n",
    "\n",
    "alpha = 1\n",
    "beta = 0.2\n",
    "omega = 0.8\n",
    "rho = 0.3\n",
    "mu = 0.7\n",
    "gamma = 0.7\n",
    "\n",
    "tau = np.ones([n_dims, n_dims]) # tau 即信息素矩阵\n",
    "eta = init_eta(Xtrain, Xtest, ytrain, ytest) # eta 即启发式信息\n",
    "print(\"init finishing\")\n",
    "feat_list = list(range(n_dims)) # feature 总list\n",
    "best_score = np.inf\n",
    "score_list = []\n",
    "acc_list = []\n",
    "\n",
    "# p_matrix = np.zeros((n_epochs, n_dims, n_dims))\n",
    "path_matrix = np.zeros((n_epochs, n_ants, n_dims))\n",
    "nf_matrix = np.zeros((n_epochs, n_ants))\n",
    "tau_matrix = np.zeros((n_epochs, n_dims, n_dims))\n",
    "best_matrix = np.zeros((n_epochs,))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # ============ apply once fs ============\n",
    "    ant_path = np.zeros([n_ants, n_dims])           # 初始化每只蚂蚁在当前迭代中的路径\n",
    "    ant_acc = np.zeros(n_ants)\n",
    "    ant_score = np.zeros(n_ants)\n",
    "    n_feats = np.random.randint(1, n_dims, size=n_ants) # 初始化每只蚂蚁需要选择的特征数\n",
    "    # n_feats = 8 * np.ones([n_ants], dtype=np.int64)\n",
    "\n",
    "    for i in range(n_ants):\n",
    "\n",
    "        ant_path[i, 0] = np.random.randint(n_dims)  # 为每只蚂蚁选择起始节点（特征）\n",
    "        # ant_path[i, 0] = i\n",
    "        visited = []                                # 已选择的 feature list\n",
    "        \n",
    "        for d in range(n_feats[i]-1):               # 共选择 n_feats-1 次特征\n",
    "            visited.append(ant_path[i, d])          # 更新 selected 表\n",
    "            # eta = update_eta(train_data, train_label, test_data, test_label, visited)\n",
    "                                                    # 更新启发式信息, eta = TPR / d, array(n_dims,)\n",
    "            p = (tau[int(visited[-1])] ** alpha) * (eta[int(visited[-1])] ** beta)\n",
    "            p[[int(i) for i in visited]] = 0\n",
    "            prob = p / sum(p)                       # 计算路径转移矩阵\n",
    "            route = select_route(prob)              # 寻找下一个特征\n",
    "            ant_path[i, d+1] = route\n",
    "\n",
    "    path_matrix[epoch] = ant_path.copy()\n",
    "    nf_matrix[epoch] = n_feats.copy()\n",
    "\n",
    "    # ==== evaluate each selected subset ====\n",
    "    for j in range(n_ants):\n",
    "        selected = list(ant_path[j, :n_feats[j]])\n",
    "        f, acc = fitness_func(Xtrain, Xtest, ytrain, ytest, selected, omega)\n",
    "                                                    # 计算适应度函数\n",
    "        ant_score[j] = f\n",
    "        ant_acc[j] = acc\n",
    "        if f <= best_score:                          # 保存为全局的最优解\n",
    "            best_path = ant_path[j]\n",
    "            best_score = f\n",
    "            best_path_acc = acc\n",
    "    \n",
    "    best_ant = np.argmin(ant_score)                 # 最优蚂蚁\n",
    "    best_matrix[epoch] = best_ant\n",
    "    near_ant = np.argmin(np.concatenate([ant_score[:best_ant], [0], ant_score[best_ant+1:]]))\n",
    "                                                    # 第二优蚂蚁\n",
    "    print(\"Epoch {} Best Score: {}, the Accuracy: {}, Num of Features: {}\".format(\\\n",
    "        epoch, ant_score[best_ant], ant_acc[best_ant], n_feats[best_ant]))\n",
    "    \n",
    "    score_list.append(ant_score[best_ant])\n",
    "    acc_list.append(ant_acc[best_ant])\n",
    "\n",
    "    # ======== update the eta matrix ========\n",
    "    \n",
    "    # stage 1 updating\n",
    "    deta_tau_k = np.zeros([n_ants, n_dims, n_dims])\n",
    "    for k in range(n_ants):\n",
    "        value = mu * ant_acc[k] + (1-mu) / n_feats[k] # 更新值\n",
    "        for m in range(n_feats[k]-1):\n",
    "            a, b = int(ant_path[k, m]), int(ant_path[k, m+1])\n",
    "            deta_tau_k[int(k), a, b] = value\n",
    "\n",
    "    deta_tau_1 = np.sum(deta_tau_k, 0)\n",
    "\n",
    "    # stage 2 updating\n",
    "    deta_tau_2 = np.zeros([n_dims, n_dims])\n",
    "    for p in range(n_feats[best_ant]-1):\n",
    "        a, b = int(ant_path[best_ant, p]), int(ant_path[best_ant, p+1])\n",
    "        deta_tau_2[a, b] = gamma * deta_tau_1[a, b]\n",
    "        \n",
    "    for p in range(n_feats[near_ant]-1):\n",
    "        a, b = int(ant_path[near_ant, p]), int(ant_path[near_ant, p+1])\n",
    "        deta_tau_2[a, b] += (1-gamma) * deta_tau_1[a, b]\n",
    "    \n",
    "    # update\n",
    "    tau = (1-rho) * tau + rho * deta_tau_1 + deta_tau_2\n",
    "    tau_matrix[epoch] = tau.copy()\n",
    "\n",
    "print(\"The Best Ant Path: \", best_path)\n",
    "print(\"The Best Score: \", best_score)\n",
    "print(\"The Accuracy use Best Path: \", best_path_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asQOCym8dvFh"
   },
   "outputs": [],
   "source": [
    "# PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9L5MN6YZNyzV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn import svm\n",
    "from numba import njit\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "dfSM=pd.read_excel('/content/Actual Data_After Outlier Detection_SMOTE_SS.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jIyKkXc6ObV3",
    "outputId": "b392a27a-93c8-4ca2-f610-612946b86cb7"
   },
   "outputs": [],
   "source": [
    "dfSM.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wxpj9wmuN4Y6",
    "outputId": "09cb5e25-7061-4eb5-ed6a-fd8bc5467d4c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import random\n",
    "import pyswarms as ps\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "y = dfSM.iloc[:,-1]\n",
    "y = y.values\n",
    "X = dfSM.drop('Target',axis=1)\n",
    "X = X.values\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = MLPClassifier()\n",
    "\n",
    "# Define objective function\n",
    "def f_per_particle(m, alpha):\n",
    "    \"\"\"Computes for the objective function per particle\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    m : numpy.ndarray\n",
    "        Binary mask that can be obtained from BinaryPSO, will\n",
    "        be used to mask features.\n",
    "    alpha: float (default is 0.5)\n",
    "        Constant weight for trading-off classifier performance\n",
    "        and number of features\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Computed objective function\n",
    "    \"\"\"\n",
    "    total_features = 23\n",
    "    # Get the subset of the features from the binary mask\n",
    "    if np.count_nonzero(m) == 0:\n",
    "        X_subset = X\n",
    "    else:\n",
    "        X_subset = X[:,m==1]\n",
    "    # Perform classification and store performance in P\n",
    "    classifier.fit(X_subset, y)\n",
    "    P = (classifier.predict(X_subset) == y).mean()\n",
    "    # Compute for the objective function\n",
    "    j = (alpha * (1.0 - P) + (1.0 - alpha) * (1 - (X_subset.shape[1] / total_features)))\n",
    "\n",
    "    return j\n",
    "\n",
    "def getK(D):\n",
    "    K = []\n",
    "    for k in range(D):\n",
    "        s = 1\n",
    "        for i in range(k):\n",
    "            s += (D-i)\n",
    "        l = ((D-k)/s)\n",
    "        K.append(l)\n",
    "    K[0] = 0.95\n",
    "    return K\n",
    "\n",
    "k = getK(10)\n",
    "\n",
    "np.random.choice(list(range(10)), 1, k).item(0)+1\n",
    "\n",
    "def initialize(D, NoP):\n",
    "    pk = getK(D)\n",
    "    k = np.random.choice(list(range(D)), 1, pk).item(0)+1\n",
    "    X=np.zeros((NoP, k))\n",
    "    V=np.zeros((NoP, k))\n",
    "    C=np.zeros((NoP, k))\n",
    "    for i in range(NoP):\n",
    "        for j in range(k):\n",
    "            X[i][j] = random.random()\n",
    "            V[i][j] = random.uniform(0,1)\n",
    "    cor = [0]*NoP\n",
    "    for i in range(NoP):\n",
    "        for j in range(k):\n",
    "            Xim = np.mean(X[i])\n",
    "            Xjm = np.mean(X[j])\n",
    "            num = (np.sum(X[i])-Xim*(k-1))*(np.sum(X[j])-Xjm*(k-1))\n",
    "            den = ((np.sum(X[i])-Xim*(k-1))**2) * ((np.sum(X[j])-Xjm*(k-1))**2)\n",
    "            C[i][j] = num/den\n",
    "        #s = np.sum(C[i])\n",
    "        #for j in range(k):\n",
    "        #    C[i][j] /= s\n",
    "        if(i != j):\n",
    "            cor[i] = np.sum(C[i])/k-1\n",
    "    s = np.sum(cor)\n",
    "    cor /= s\n",
    "    return cor\n",
    "\n",
    "initialize(8, 6)\n",
    "\n",
    "def f(x, alpha=0.88):\n",
    "    \"\"\"Higher-level method to do classification in the\n",
    "    whole swarm.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    x: numpy.ndarray of shape (n_particles, dimensions)\n",
    "        The swarm that will perform the search\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray of shape (n_particles, )\n",
    "        The computed loss for each particle\n",
    "    \"\"\"\n",
    "    n_particles = x.shape[0]\n",
    "    j = [f_per_particle(x[i], alpha) for i in range(n_particles)]\n",
    "    return np.array(j)\n",
    "\n",
    "\n",
    "# Initialize swarm, arbitrary\n",
    "options = {'c1': 0.5, 'c2': 0.5, 'w':0.9, 'k': 30, 'p':2}\n",
    "\n",
    "# Call instance of PSO\n",
    "dimensions = 23 # dimensions should be the number of features\n",
    "optimizer = ps.discrete.BinaryPSO(n_particles=30, dimensions=dimensions, options=options)\n",
    "\n",
    "# Perform optimization\n",
    "cost, pos = optimizer.optimize(f, iters=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8o0rqYwpOkF9",
    "outputId": "7de8632b-197e-4641-c275-b7e2b1dacdff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset performance: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier()\n",
    "X_selected_features = X[:,pos==1] \n",
    "classifier.fit(X_selected_features, y)\n",
    "subset_performance = (classifier.predict(X_selected_features) == y).mean()\n",
    "print('Subset performance: %.3f' % (subset_performance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uN82uPp6d-fx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ant colony.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
